{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Dataset for Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
    "from datasets import Dataset, load_from_disk\n",
    "from transformers import AutoTokenizer\n",
    "from multiprocessing import Pool\n",
    "from utils.preprocessing import *\n",
    "from utils.database import *\n",
    "from utils.files import *\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code `os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"` disables parallel tokenization in HuggingFace's libraries. It's a way to suppress warnings and prevent potential issues tied to multi-core tokenization.\n",
    "See: https://stackoverflow.com/questions/62691279/how-to-disable-tokenizers-parallelism-true-false-warning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "# transformers.utils.logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Raw Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_from_disk('../data/input/articles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "describeDataset(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROMPT_TEMPLATE = \"Output a response given the Output rules and Article.\\nOutput Rules: Identify if\" \\\n",
    "#     \" there is one, multiple, or zero {elt}s in the article.\\nIf the number of {elt}s == 0, then output \" \\\n",
    "#     \"'None'.\\nIf the number of {elt}s > 0, then output the names of the {elt}s as a python list.\\n\" \\\n",
    "#     \"Article: {article_text}\"\n",
    "\n",
    "PROMPT_TEMPLATE = \"Who is the {elt} in the following text?\\nText: {article_text}\"\n",
    "\n",
    "# Test the template with a dummy text\n",
    "print(PROMPT_TEMPLATE.format(elt='hero',\n",
    "      article_text='Lorem ipsum dolor sit amet, consectetur adipiscing elit.'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expand Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-large\")\n",
    "print(\"Input window length:\", tokenizer.model_max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions to segment articles into chunks fitting within the input window:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcInputLength(prompt):\n",
    "    \"\"\"Calculate the length of the input after\"\"\"\n",
    "    return tokenizer(prompt, return_tensors=\"pt\").input_ids.real.shape[1]\n",
    "\n",
    "\n",
    "template_length = calcInputLength(\n",
    "    PROMPT_TEMPLATE.format(elt='villain', article_text=' '))\n",
    "print(template_length)\n",
    "\n",
    "chunk_size = tokenizer.model_max_length - template_length\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=30,\n",
    "    # separators=['.', '?', '!', \"\\n\\n\", \"\\n\", \" \", \"\"],\n",
    "    length_function=calcInputLength)\n",
    "\n",
    "\n",
    "def split_text(text, n_tokens, tokenizer, overlap=10):\n",
    "    \"\"\"Splits the input text into chunks with n_tokens tokens using HuggingFace tokenizer, with an overlap of overlap tokens.\"\"\"\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    chunks = []\n",
    "    i = 0\n",
    "    while i < len(tokens):\n",
    "        chunk = tokens[i:i+n_tokens]\n",
    "        chunks.append(tokenizer.convert_tokens_to_string(chunk))\n",
    "        i += n_tokens - overlap\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each article, distinct prompts identify 'hero', 'villain', and 'victim'. If an article exceeds the model's input size, it's divided into chunks, generating additional prompts. It seems that one article results in about 10 to 12 prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expandRow(row):\n",
    "    \"\"\"\n",
    "    Generate prompts based on various roles and text chunks from the input row.\n",
    "    \"\"\"\n",
    "    roles = ['hero', 'villain', 'victim']\n",
    "    prompts = []\n",
    "\n",
    "    # Split the text into chunks\n",
    "    # text_splitter.split_text(row.get('text'))\n",
    "    text_chunks = split_text(row.get('text'), 450, tokenizer, overlap=10)\n",
    "\n",
    "    # Generate prompts for each role and text chunk\n",
    "    for role in roles:\n",
    "        for chunk_id, text_chunk in enumerate(text_chunks):\n",
    "            prompt = PROMPT_TEMPLATE.format(elt=role, article_text=text_chunk)\n",
    "            new_row = {\n",
    "                **row,\n",
    "                'prompt': prompt,\n",
    "                'role': role,\n",
    "                'chunk': chunk_id,\n",
    "                'chunk_length': calcInputLength(text_chunk)\n",
    "            }\n",
    "            prompts.append(new_row)\n",
    "\n",
    "    return prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process datataset using multiple proesses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_processes = 12\n",
    "\n",
    "with Pool(processes=num_processes) as pool:\n",
    "    # The pool.map function applies the expandRow function to each row in dataset\n",
    "    # and returns a list of results. Each result is a list, so we flatten the list using itertools.chain.\n",
    "    dataset_hvv = list(pool.map(expandRow, dataset))\n",
    "\n",
    "# Flatten the resulting list of lists\n",
    "dataset_hvv = [item for sublist in dataset_hvv for item in sublist]\n",
    "\n",
    "# Convert the list of dictionaries into a Dataset\n",
    "dataset_hvv = Dataset.from_dict(\n",
    "    {key: [dic[key] for dic in dataset_hvv] for key in dataset_hvv[0]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_hvv.save_to_disk('data/input/articles_chunkified')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizeInputs(example):\n",
    "    \"\"\"Tokenize the inputs\"\"\"\n",
    "\n",
    "    tokenized_inputs = tokenizer(example[\"prompt\"], max_length=tokenizer.model_max_length,\n",
    "                                    truncation=True, is_split_into_words=False, add_special_tokens=True, padding=\"max_length\")\n",
    "\n",
    "    # Combine original data with the tokenized inputs\n",
    "    example.update(tokenized_inputs)\n",
    "    return example\n",
    "\n",
    "tokenized_dataset = dataset_hvv.map(tokenizeInputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate some basic Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_prompt_length(row):\n",
    "    row['prompt_length'] = calcInputLength(row['prompt'])\n",
    "    return row\n",
    "\n",
    "# Assuming the dataset object supports a map operation\n",
    "tokenized_dataset = tokenized_dataset.map(calculate_prompt_length)\n",
    "\n",
    "# Assuming the dataset object can be iterated like a list\n",
    "min_length = min(row['prompt_length'] for row in tokenized_dataset)\n",
    "max_length = max(row['prompt_length'] for row in tokenized_dataset)\n",
    "total_length = sum(row['prompt_length'] for row in tokenized_dataset)\n",
    "avg_length = total_length / len(tokenized_dataset)\n",
    "\n",
    "print(\"Minimum prompt length:\", min_length)\n",
    "print(\"Maximum prompt length:\", max_length)\n",
    "print(\"Average prompt length:\", avg_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenized_dataset[0][\"prompt\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Dataset to Disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset.save_to_disk('../data/input/articles_tokenized')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mediacloud",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
