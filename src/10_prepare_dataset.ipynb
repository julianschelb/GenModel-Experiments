{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Dataset for Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jschelb/.pyenv/versions/3.10.8/envs/mediacloud/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
    "from datasets import Dataset, load_from_disk\n",
    "from transformers import AutoTokenizer\n",
    "from multiprocessing import Pool\n",
    "from utils.preprocessing import *\n",
    "from utils.database import *\n",
    "from utils.files import *\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code `os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"` disables parallel tokenization in HuggingFace's libraries. It's a way to suppress warnings and prevent potential issues tied to multi-core tokenization.\n",
    "See: https://stackoverflow.com/questions/62691279/how-to-disable-tokenizers-parallelism-true-false-warning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "# transformers.utils.logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Raw Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_from_disk('../data/input/articles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 50\n",
      "Column names: ['id', 'title', 'url', 'text']\n",
      "Features (schema): {'id': Value(dtype='string', id=None), 'title': Value(dtype='string', id=None), 'url': Value(dtype='string', id=None), 'text': Value(dtype='string', id=None)}\n"
     ]
    }
   ],
   "source": [
    "describeDataset(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Who is the hero in the following text?\n",
      "Text: Lorem ipsum dolor sit amet, consectetur adipiscing elit.\n"
     ]
    }
   ],
   "source": [
    "# PROMPT_TEMPLATE = \"Output a response given the Output rules and Article.\\nOutput Rules: Identify if\" \\\n",
    "#     \" there is one, multiple, or zero {elt}s in the article.\\nIf the number of {elt}s == 0, then output \" \\\n",
    "#     \"'None'.\\nIf the number of {elt}s > 0, then output the names of the {elt}s as a python list.\\n\" \\\n",
    "#     \"Article: {article_text}\"\n",
    "\n",
    "PROMPT_TEMPLATE = \"Who is the {elt} in the following text?\\nText: {article_text}\"\n",
    "\n",
    "# Test the template with a dummy text\n",
    "print(PROMPT_TEMPLATE.format(elt='hero',\n",
    "      article_text='Lorem ipsum dolor sit amet, consectetur adipiscing elit.'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expand Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input window length: 512\n"
     ]
    }
   ],
   "source": [
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-large\")\n",
    "print(\"Input window length:\", tokenizer.model_max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions to segment articles into chunks fitting within the input window:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n"
     ]
    }
   ],
   "source": [
    "def calcInputLength(prompt):\n",
    "    \"\"\"Calculate the length of the input after\"\"\"\n",
    "    return tokenizer(prompt, return_tensors=\"pt\").input_ids.real.shape[1]\n",
    "\n",
    "\n",
    "template_length = calcInputLength(\n",
    "    PROMPT_TEMPLATE.format(elt='villain', article_text=' '))\n",
    "print(template_length)\n",
    "\n",
    "chunk_size = tokenizer.model_max_length - template_length\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=30,\n",
    "    # separators=['.', '?', '!', \"\\n\\n\", \"\\n\", \" \", \"\"],\n",
    "    length_function=calcInputLength)\n",
    "\n",
    "\n",
    "def split_text(text, n_tokens, tokenizer, overlap=10):\n",
    "    \"\"\"Splits the input text into chunks with n_tokens tokens using HuggingFace tokenizer, with an overlap of overlap tokens.\"\"\"\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    chunks = []\n",
    "    i = 0\n",
    "    while i < len(tokens):\n",
    "        chunk = tokens[i:i+n_tokens]\n",
    "        chunks.append(tokenizer.convert_tokens_to_string(chunk))\n",
    "        i += n_tokens - overlap\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each article, distinct prompts identify 'hero', 'villain', and 'victim'. If an article exceeds the model's input size, it's divided into chunks, generating additional prompts. It seems that one article results in about 10 to 12 prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expandRow(row):\n",
    "    \"\"\"\n",
    "    Generate prompts based on various roles and text chunks from the input row.\n",
    "    \"\"\"\n",
    "    roles = ['hero', 'villain', 'victim']\n",
    "    prompts = []\n",
    "\n",
    "    # Split the text into chunks\n",
    "    # text_splitter.split_text(row.get('text'))\n",
    "    text_chunks = split_text(row.get('text'), 450, tokenizer, overlap=10)\n",
    "\n",
    "    # Generate prompts for each role and text chunk\n",
    "    for role in roles:\n",
    "        for chunk_id, text_chunk in enumerate(text_chunks):\n",
    "            prompt = PROMPT_TEMPLATE.format(elt=role, article_text=text_chunk)\n",
    "            new_row = {\n",
    "                **row,\n",
    "                'prompt': prompt,\n",
    "                'role': role,\n",
    "                'chunk': chunk_id,\n",
    "                'chunk_length': calcInputLength(text_chunk)\n",
    "            }\n",
    "            prompts.append(new_row)\n",
    "\n",
    "    return prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process datataset using multiple proesses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (652 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (839 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1109 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1058 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (673 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2818 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2183 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3672 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (966 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (7650 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (940 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (7975 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "num_processes = 12\n",
    "\n",
    "with Pool(processes=num_processes) as pool:\n",
    "    # The pool.map function applies the expandRow function to each row in dataset\n",
    "    # and returns a list of results. Each result is a list, so we flatten the list using itertools.chain.\n",
    "    dataset_hvv = list(pool.map(expandRow, dataset))\n",
    "\n",
    "# Flatten the resulting list of lists\n",
    "dataset_hvv = [item for sublist in dataset_hvv for item in sublist]\n",
    "\n",
    "# Convert the list of dictionaries into a Dataset\n",
    "dataset_hvv = Dataset.from_dict(\n",
    "    {key: [dic[key] for dic in dataset_hvv] for key in dataset_hvv[0]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 651/651 [00:00<00:00, 17371.97 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset_hvv.save_to_disk('data/input/articles_chunkified')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 651/651 [00:00<00:00, 740.17 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def tokenizeInputs(example):\n",
    "    \"\"\"Tokenize the inputs\"\"\"\n",
    "\n",
    "    tokenized_inputs = tokenizer(example[\"prompt\"], max_length=tokenizer.model_max_length,\n",
    "                                    truncation=True, is_split_into_words=False, add_special_tokens=True, padding=\"max_length\")\n",
    "\n",
    "    # Combine original data with the tokenized inputs\n",
    "    example.update(tokenized_inputs)\n",
    "    return example\n",
    "\n",
    "tokenized_dataset = dataset_hvv.map(tokenizeInputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate some basic Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 651/651 [00:00<00:00, 740.70 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum prompt length: 37\n",
      "Maximum prompt length: 464\n",
      "Average prompt length: 408.27342549923196\n"
     ]
    }
   ],
   "source": [
    "def calculate_prompt_length(row):\n",
    "    row['prompt_length'] = calcInputLength(row['prompt'])\n",
    "    return row\n",
    "\n",
    "# Assuming the dataset object supports a map operation\n",
    "tokenized_dataset = tokenized_dataset.map(calculate_prompt_length)\n",
    "\n",
    "# Assuming the dataset object can be iterated like a list\n",
    "min_length = min(row['prompt_length'] for row in tokenized_dataset)\n",
    "max_length = max(row['prompt_length'] for row in tokenized_dataset)\n",
    "total_length = sum(row['prompt_length'] for row in tokenized_dataset)\n",
    "avg_length = total_length / len(tokenized_dataset)\n",
    "\n",
    "print(\"Minimum prompt length:\", min_length)\n",
    "print(\"Maximum prompt length:\", max_length)\n",
    "print(\"Average prompt length:\", avg_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Who is the hero in the following text?\n",
      "Text: मेरा शहर Link Copied रहें हर खबर से अपडेट, डाउनलोड करें Android Hindi News App, iOS Hindi News App और Amarujala Hindi News APP अपने मोबाइल पे|Get all India News in Hindi related to live update of politics, sports, entertainment, technology and education etc. Stay updated with us for all breaking news from India News and more news in Hindi. Next Article Please wait... Please wait... Delete All Cookies Followed\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_dataset[0][\"prompt\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Dataset to Disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 651/651 [00:00<00:00, 9867.88 examples/s] \n"
     ]
    }
   ],
   "source": [
    "tokenized_dataset.save_to_disk('../data/input/articles_tokenized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mediacloud",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
