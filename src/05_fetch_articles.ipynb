{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetch Articles from Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.preprocessing import *\n",
    "from utils.database import *\n",
    "from utils.files import *\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Credentials are sourced from the `.env` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, db = getConnection(use_dotenv=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetches a limited number of articles from the database that haven't been processed yet, \n",
    "returning specified fields like url, title, and parsing result text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = {\"url\": 1, \"title\": 1, \"parsing_result.text\": 1}\n",
    "articles = fetchArticleTexts(db,  limit=10_000, skip=0, fields=fields, query={\n",
    "                             \"processing_result\": {\"$exists\": False}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Processes the 'parsing_result' of each article to clean the text, and filters out articles \n",
    "that lack a 'title' or 'parsing_result'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for article in articles:\n",
    "    text = article.get(\"parsing_result\", {}).get(\"text\", \"\")\n",
    "    if \"parsing_result\" in article:\n",
    "        article[\"parsing_result\"][\"text\"] = cleanText(text)\n",
    "    else:\n",
    "        print(\"No parsing result for article\", article[\"_id\"])\n",
    "\n",
    "articles = [article for article in articles if article.get(\n",
    "    \"title\", \"\") and article.get(\"parsing_result\", \"\")]\n",
    "\n",
    "print(\"Number of articles:\", len(articles))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export as JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saves the given data to a JSON file for optional visual inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exportAsJSON(\"../data/input/articles.json\",  articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert article IDs to strings and transform a list of articles into a dataset with fields: id, title, url, and text extracted from parsing results. The HuggingFace `datasets` library provides several key advantages over plain JSON files:\n",
    "\n",
    "- **Efficiency**: The datasets are memory-mapped, allowing you to work with data that's larger than your available RAM without loading the entire dataset into memory. \n",
    "- **Speed**: Datasets in the HuggingFace format (which is Arrow-based) can be loaded faster than large JSON files, facilitating quicker data operations.\n",
    "- **Columnar Storage**: By using Apache Arrow for storage, HuggingFace datasets benefit from a columnar format that ensures more efficient serialization and deserialization compared to row-based storage, such as JSON.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through each article in the list of articles\n",
    "# and convert the \"_id\" field of the article to a string\n",
    "for article in articles:\n",
    "    article[\"_id\"] = str(article[\"_id\"])\n",
    "\n",
    "# Convert the list of JSON objects into a dictionary format\n",
    "dataset_dict = {\n",
    "    \"id\": [_[\"_id\"] for _ in articles],\n",
    "    \"title\": [_[\"title\"] for _ in articles],\n",
    "    \"url\": [_[\"url\"] for _ in articles],\n",
    "    \"text\": [_[\"parsing_result\"][\"text\"] for _ in articles],\n",
    "}\n",
    "\n",
    "dataset = Dataset.from_dict(dataset_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save dataset to disk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.save_to_disk('../data/input/articles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset[0][\"text\"][:100])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mediacloud",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
